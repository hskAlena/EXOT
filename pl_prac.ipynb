{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterlab\n",
    "!pip install wandb pytorch-lightning\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Weights & Biases\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Pytorch-Lightning\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Dataset\n",
    "from torchvision import transforms\n",
    "# create local file path \n",
    "!python tracking/create_default_local_file.py --workspace_dir . --data_dir ./data --save_dir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.misc import NestedTensor\n",
    "from lib.utils.box_ops import box_cxcywh_to_xyxy, box_xywh_to_xyxy\n",
    "from lib.utils.merge import merge_template_search\n",
    "\n",
    "import argparse\n",
    "from lib.train.admin.environment import env_settings\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "import cv2 as cv\n",
    "from lib.train.base_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/wandb_fc/korean/reports/Weights-Biases-Pytorch-Lightning---VmlldzozNzAxOTg\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/latest/cli/lightning_cli_advanced.html\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/latest/starter/converting.html\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/1.4.0/advanced/multi_gpu.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    \"\"\"\n",
    "    args for training.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Parse args for training')\n",
    "    # for train\n",
    "    parser.add_argument('--script', type=str, help='training script name')\n",
    "    parser.add_argument('--config', type=str, default='baseline', help='yaml configure file name')\n",
    "    parser.add_argument('--save_dir', type=str, help='root directory to save checkpoints, logs, and tensorboard')\n",
    "    parser.add_argument('--mode', type=str, choices=[\"single\", \"multiple\"], default=\"multiple\",\n",
    "                        help=\"train on single gpu or multiple gpus\")\n",
    "    parser.add_argument('--cudnn_benchmark', type=bool, default=True, help='Set cudnn benchmark on (1) or off (0) (default is on).')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int, help='node rank for distributed training')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=42, help='seed for random numbers')\n",
    "    parser.add_argument('--dry_run', type=int, default=1, help='0: wandb activate, 1: wandb off')\n",
    "    parser.add_argument('--nproc_per_node', type=int, help=\"number of GPUs per node\")  # specify when mode is multiple\n",
    "    parser.add_argument('--use_lmdb', type=int, choices=[0, 1], default=0)  # whether datasets are in lmdb format\n",
    "    parser.add_argument('--script_prv', type=str, help='training script name')\n",
    "    parser.add_argument('--config_prv', type=str, default='baseline', help='yaml configure file name')\n",
    "\n",
    "    args = parser.parse_args(args)\n",
    "\n",
    "    return args\n",
    "\n",
    "class Settings:\n",
    "    \"\"\" Training settings, e.g. the paths to datasets and networks.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.set_default()       \n",
    "\n",
    "    def set_default(self):\n",
    "        self.env = env_settings()\n",
    "        self.use_gpu = True\n",
    "\n",
    "    def set_args(self, args):\n",
    "        # self.args = args\n",
    "        self.script_name = args.script\n",
    "        self.config_name = args.config\n",
    "        self.dry_run = args.dry_run\n",
    "        self.project_path = 'train/{}/{}'.format(self.script_name, self.config_name)\n",
    "        if args.script_prv is not None and args.config_prv is not None:\n",
    "            self.project_path_prv = 'train/{}/{}'.format(args.script_prv, args.config_prv)\n",
    "        self.local_rank = args.local_rank\n",
    "        self.save_dir = os.path.abspath(args.save_dir)\n",
    "        self.use_lmdb = args.use_lmdb\n",
    "        prj_dir = os.path.abspath('')\n",
    "        # prj_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\n",
    "        self.cfg_file = os.path.join(prj_dir, 'experiments/%s/%s.yaml' % (self.script_name, self.config_name))\n",
    "\n",
    "        self.description = 'Training script for STARK-S, STARK-ST stage1, and STARK-ST stage2'\n",
    "\n",
    "        # update the default configs with config file\n",
    "        if not os.path.exists(self.cfg_file):\n",
    "            raise ValueError(\"%s doesn't exist.\" % self.cfg_file)\n",
    "        config_module = importlib.import_module(\"lib.config.%s.config\" % self.script_name)\n",
    "        cfg = config_module.cfg\n",
    "        config_module.update_config_from_file(self.cfg_file)     \n",
    "\n",
    "        # Record the training log\n",
    "        log_dir = os.path.join(self.save_dir, 'logs')\n",
    "        if self.local_rank in [-1, 0]:\n",
    "            if not os.path.exists(log_dir):\n",
    "                os.makedirs(log_dir)\n",
    "        self.log_file = os.path.join(log_dir, \"%s-%s.log\" % (self.script_name, self.config_name))\n",
    "        return cfg\n",
    "\n",
    "def init_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitEXOTActor(LightningModule):\n",
    "    def __init__(self, cfg, settings, loss_type, lr =0.0001):\n",
    "        '''method used to define our model parameters'''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.settings = settings\n",
    "        self.bs = self.settings.batchsize  # batch size\n",
    "        self.exit_flag = loss_type\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.cfg = cfg\n",
    "        self.lr = lr\n",
    "\n",
    "        # metrics\n",
    "        # self.accuracy = pl.metrics.Accuracy()\n",
    "\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def construct(self, net, objective, loss_weight):\n",
    "        self.net = net\n",
    "        self.objective = objective\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "\n",
    "    def forward(self, data, run_box_head=True, run_cls_head=False):\n",
    "        feat_dict_list = []\n",
    "        # process the templates\n",
    "        for i in range(self.settings.num_template):\n",
    "            template_img_i = data['template_images'][i].view(-1, *data['template_images'].shape[2:])  # (batch, 3, 128, 128)\n",
    "            template_att_i = data['template_att'][i].view(-1, *data['template_att'].shape[2:])  # (batch, 128, 128)\n",
    "            feat_dict_list.append(self.net(img=NestedTensor(template_img_i, template_att_i), mode='backbone'))\n",
    "\n",
    "        # process the search regions (t-th frame)\n",
    "        search_img = data['search_images'].view(-1, *data['search_images'].shape[2:])  # (batch, 3, 320, 320)\n",
    "        search_att = data['search_att'].view(-1, *data['search_att'].shape[2:])  # (batch, 320, 320)\n",
    "        feat_dict_list.append(self.net(img=NestedTensor(search_img, search_att), mode='backbone'))\n",
    "\n",
    "        # run the transformer and compute losses\n",
    "        seq_dict = merge_template_search(feat_dict_list)\n",
    "        \n",
    "        template_bboxes = box_xywh_to_xyxy(data['template_anno'])  #(N_t, batch, 4)\n",
    "\n",
    "        # search_joint = data['search_joint'] #(N_s, batch, 6)\n",
    "        # print('joint flag', data['joint_flag'])\n",
    "        # if data['joint_flag'][0] != 'None':\n",
    "        #     template_joint = data['template_joint'] #(N_t, batch, 6)\n",
    "        # else:\n",
    "        template_joint = None\n",
    "        joint_annot = (template_bboxes, template_joint)  # template anno, template joint\n",
    "        out_dict, _, _, flagFeat = self.net(seq_dict=seq_dict, annot = joint_annot, mode=\"transformer\", run_box_head=run_box_head, run_cls_head=run_cls_head)\n",
    "        # out_dict: (B, N, C), outputs_coord: (1, B, N, C), target_query: (1, B, N, C)\n",
    "        return out_dict, flagFeat\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "        # data, y = batch\n",
    "\n",
    "        out_dict, flagFeat = self(data, run_box_head=True, run_cls_head=False)\n",
    "        gt_exit, gt_package, gt_bboxes = self.process_gt(data)\n",
    "\n",
    "        # compute losses\n",
    "        if flagFeat == None:\n",
    "            flagFeat = (gt_exit, gt_package) #, data['epoch'])\n",
    "        else:\n",
    "            exitflag, feature = flagFeat\n",
    "            flagFeat = (gt_exit, gt_package, feature, exitflag) #, data['epoch'])\n",
    "        loss, status = self.compute_losses(out_dict, gt_bboxes, flag_feat = flagFeat)\n",
    "        \n",
    "        # return loss, status\n",
    "\n",
    "        # Log training loss\n",
    "        self.log('Loss/train_total', loss)\n",
    "        self.log('train_batch_stepidx', batch_idx)\n",
    "\n",
    "        # Log metrics\n",
    "\n",
    "        self.log('Loss/train_giou', status['Loss/giou'])\n",
    "        self.log('Loss/train_l1', status['Loss/l1'])\n",
    "        self.log('train_IoU', status['IoU'])\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        # data, y = batch\n",
    "        # data['epoch'] = self.epoch\n",
    "        # data['settings'] = self.settings\n",
    "\n",
    "        out_dict, flagFeat = self(data, run_box_head=True, run_cls_head=False)\n",
    "        gt_exit, gt_package, gt_bboxes = self.process_gt(data)\n",
    "\n",
    "        # compute losses\n",
    "        if flagFeat == None:\n",
    "            flagFeat = (gt_exit, gt_package) #, data['epoch'])\n",
    "        else:\n",
    "            exitflag, feature = flagFeat\n",
    "            flagFeat = (gt_exit, gt_package, feature, exitflag) #, data['epoch'])\n",
    "        loss, status = self.compute_losses(out_dict, gt_bboxes, flag_feat = flagFeat)\n",
    "\n",
    "        # Log validation loss (will be automatically averaged over an epoch)\n",
    "        # Log training loss\n",
    "        self.log('Loss/valid_total', loss)\n",
    "        self.log('val_batch_stepidx', batch_idx)\n",
    "\n",
    "        # Log metrics\n",
    "\n",
    "        self.log('Loss/valid_giou', status['Loss/giou'])\n",
    "        self.log('Loss/valid_l1', status['Loss/l1'])\n",
    "        self.log('valid_IoU', status['IoU'])\n",
    "    \n",
    "    def test_step(self, data, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        # data, y = batch\n",
    "\n",
    "        out_dict, flagFeat = self(data, run_box_head=True, run_cls_head=False)\n",
    "        gt_exit, gt_package, gt_bboxes = self.process_gt(data)\n",
    "\n",
    "        # compute losses\n",
    "        if flagFeat == None:\n",
    "            flagFeat = (gt_exit, gt_package) #, data['epoch'])\n",
    "        else:\n",
    "            exitflag, feature = flagFeat\n",
    "            flagFeat = (gt_exit, gt_package, feature, exitflag) #, data['epoch'])\n",
    "        loss, status = self.compute_losses(out_dict, gt_bboxes, flag_feat = flagFeat)\n",
    "\n",
    "        # Log test loss\n",
    "        self.log('Loss/test_total', loss)\n",
    "        self.log('test_batch_stepidx', batch_idx)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('Loss/test_giou', status['Loss/giou'])\n",
    "        self.log('Loss/test_l1', status['Loss/l1'])\n",
    "        self.log('test_IoU', status['IoU'])\n",
    "\n",
    "        # Log metrics\n",
    "        #self.log('test_acc', self.accuracy(logits, y))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''defines model optimizer'''\n",
    "        optimizer, lr_scheduler = self.get_optimizer_scheduler(self.cfg)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def process_gt(self, data):\n",
    "        # process the groundtruth\n",
    "        gt_bboxes = data['search_anno']  # (Ns, batch, 4) (x1,y1,w,h)\n",
    "\n",
    "        # if data['joint_flag'][0] != 'None':            \n",
    "        #     gt_joints = torch.squeeze(data['search_joint'])\n",
    "        #     gt_depth = torch.squeeze(data['search_depth'])\n",
    "        #     template_depth = data['template_depth']\n",
    "        #     template_bboxes = box_xywh_to_xyxy(data['template_anno']) \n",
    "        #     gt_package = [gt_joints, gt_depth, template_depth, template_bboxes]\n",
    "        # else:\n",
    "        template_bboxes = box_xywh_to_xyxy(data['template_anno']) \n",
    "        gt_package = [template_bboxes]\n",
    "\n",
    "\n",
    "        gt_exit = torch.squeeze(data['search_exit'])\n",
    "        # compute losses\n",
    "        if gt_bboxes.dim() ==3 and gt_bboxes.shape[0]==1:\n",
    "            gt_bboxes = gt_bboxes[0]\n",
    "        return gt_exit, gt_package, gt_bboxes\n",
    "\n",
    "    def compute_losses(self, pred_dict, gt_bbox, flag_feat = None, return_status=True):\n",
    "        if len(flag_feat) == 2:\n",
    "            gt_exit, gt_package = flag_feat #, epoch\n",
    "            exitflag = None; feature = None\n",
    "        else:\n",
    "            gt_exit, gt_package, feature, exitflag = flag_feat # , epoch\n",
    "        \n",
    "        # Get boxes\n",
    "        pred_boxes = pred_dict['pred_boxes']\n",
    "        if torch.isnan(pred_boxes).any():\n",
    "            raise ValueError(\"Network outputs is NAN! Stop Training\")\n",
    "        num_queries = pred_boxes.size(1)\n",
    "\n",
    "        pred_bboxes_vec = box_cxcywh_to_xyxy(pred_boxes)\n",
    "        pred_boxes_vec = pred_bboxes_vec.view(-1, 4) # (B,N,4) --> (BN,4) (x1,y1,x2,y2)\n",
    "        if gt_bbox.dim() ==3:\n",
    "            gt_bboxes_vec = box_xywh_to_xyxy(gt_bbox).clamp(min=-1.0, max=1.0)  # (B,4) --> (B,1,4) --> (B,N,4)\n",
    "            gt_boxes_vec = gt_bboxes_vec.view(-1,4).clamp(min=0.0, max=1.0)\n",
    "            \n",
    "            n, b, _ = gt_bbox.shape\n",
    "            neg_flags = torch.zeros(n, b).detach() #.cuda()\n",
    "            nonzero = torch.nonzero(gt_bbox<0, as_tuple=True)\n",
    "            for i in range(len(nonzero[0])):\n",
    "                neg_flags[nonzero[0][i]][nonzero[1][i]] = 1\n",
    "        else:\n",
    "            tmp = box_xywh_to_xyxy(gt_bbox)\n",
    "            gt_boxes_vec = box_xywh_to_xyxy(gt_bbox)[:, None, :]\n",
    "            gt_bboxes_vec = gt_boxes_vec.repeat((1, num_queries, 1)).clamp(min=-1.0, max=1.0)\n",
    "            gt_boxes_vec = gt_bboxes_vec.view(-1, 4).clamp(min=0.0, max=1.0)              # (B,4) --> (B,1,4) --> (B,N,4)\n",
    "            \n",
    "            neg_flags = torch.zeros(self.bs).detach() #.cuda()\n",
    "            nonzero = torch.nonzero(gt_bbox<0, as_tuple=True)\n",
    "\n",
    "            for i in range(len(nonzero[0])):\n",
    "                # print(\"GT NEG\", gt_bboxes_vec)\n",
    "                neg_flags[nonzero[0][i]] = 1\n",
    "\n",
    "        pred_boxes_vec = box_cxcywh_to_xyxy(pred_boxes).view(-1, 4)  # (B,N,4) --> (BN,4) (x1,y1,x2,y2)\n",
    "        gt_boxes_vec = box_xywh_to_xyxy(gt_bbox)[:, None, :].repeat((1, num_queries, 1)).view(-1, 4).clamp(min=0.0, max=1.0)  # (B,4) --> (B,1,4) --> (B,N,4)\n",
    "        # compute giou and iou\n",
    "        try:\n",
    "            giou_loss, iou = self.objective['giou'](pred_boxes_vec, gt_boxes_vec)  # (BN,4) (BN,4)\n",
    "        except:\n",
    "            giou_loss, iou = torch.tensor(0.0), torch.tensor(0.0) #.cuda()\n",
    "        # compute l1 loss\n",
    "        l1_loss = self.objective['l1'](pred_boxes_vec, gt_boxes_vec)  # (BN,4) (BN,4)\n",
    "        \n",
    "        # compute exit loss\n",
    "        if self.exit_flag == 'None':\n",
    "            exit_loss = torch.tensor(0.0) #.cuda()\n",
    "            reid_loss = torch.tensor(0.0)\n",
    "            joint_loss = torch.tensor(0.0)\n",
    "        else:\n",
    "            exit_loss = self.compute_exit_loss(self, exitflag, neg_flags, gt_bboxes_vec, gt_exit)\n",
    "            joint_loss, l1_loss = self.compute_joint_loss(self, gt_bbox, pred_dict, gt_package)\n",
    "            reid_loss = self.compute_reid_loss(gt_bbox, pred_bboxes_vec, feature)\n",
    "\n",
    "\n",
    "        # weighted sum\n",
    "\n",
    "        # if self.exit_flag == 'None':\n",
    "        loss = self.loss_weight['giou'] * giou_loss + self.loss_weight['l1'] * l1_loss\n",
    "        # else:\n",
    "        #     if epoch<3:\n",
    "        #         loss = self.loss_weight['exit']*exit_loss\n",
    "        #         #loss = self.loss_weight['giou'] * giou_loss + self.loss_weight['l1'] * l1_loss\n",
    "        #     elif epoch<5:\n",
    "        #         loss = self.loss_weight['giou'] * giou_loss + self.loss_weight['l1'] * l1_loss + self.loss_weight['exit']*exit_loss                 \n",
    "        #     else:\n",
    "        #         loss = self.loss_weight['giou'] * giou_loss + self.loss_weight['l1'] * l1_loss \\\n",
    "        #             + self.loss_weight['exit']*exit_loss + self.loss_weight['reId']*reid_loss\n",
    "\n",
    "        \n",
    "        if return_status:\n",
    "            # status for log\n",
    "            mean_iou = iou.detach().mean()\n",
    "            # status = {\"Loss/total\": loss.item(),\n",
    "            #           \"Loss/giou\": giou_loss.item(),\n",
    "            #           \"Loss/l1\": l1_loss.item(),\n",
    "            #           \"IoU\": mean_iou.item()}\n",
    "\n",
    "            status = {\"Loss/total\": loss.item(),\n",
    "                          \"Loss/giou\": giou_loss.item(),\n",
    "                          \"Loss/l1\": l1_loss.item(),\n",
    "                          \"IoU\": mean_iou.item(),\n",
    "                          \"Loss/joint\": joint_loss.item(),\n",
    "                          \"Loss/reId\": reid_loss.item(),\n",
    "                          \"Loss/exit\": exit_loss.item()}\n",
    "            return loss, status\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def compute_joint_loss(self, gt_bbox, pred_dict, gt_package):\n",
    "        if len(gt_package)>1:\n",
    "            # print(gt_package[0].shape, gt_package[1].shape, gt_package[2].shape)\n",
    "            # torch.Size([3, 6]) torch.Size([3, 480, 640]) torch.Size([2, 3, 480, 640])\n",
    "            [gt_joints, gt_depth, template_depth, template_bboxes] = gt_package\n",
    "            if gt_bbox.dim()==3:\n",
    "                gt_joints = gt_joints[:,:,:2].view(-1, 2)\n",
    "            else:\n",
    "                gt_joints = gt_joints[:, :2]\n",
    "            pred_joints = pred_dict['pred_joint']\n",
    "            # print(gt_joints.shape, pred_joints.shape)\n",
    "            if pred_joints !=None:\n",
    "                joint_loss = self.objective['joint'](gt_joints, pred_joints)\n",
    "                l1_loss = l1_loss + joint_loss/2\n",
    "            else:\n",
    "                joint_loss = torch.tensor(0.0)\n",
    "            template_bboxes = template_bboxes.clamp(min=-1.0, max=1.0) \n",
    "            #reid_loss += self.compute_reid_depth(pred_bboxes_vec, template_bboxes, gt_depth, template_depth)\n",
    "            #reid_loss = reid_loss/2\n",
    "        else:\n",
    "            joint_loss = torch.tensor(0.0)\n",
    "        return joint_loss, l1_loss\n",
    "\n",
    "    def compute_exit_loss(self, exitflag, neg_flags, gt_bboxes_vec, gt_exit):\n",
    "        if exitflag == None:\n",
    "            exit_loss = torch.tensor(0.0) #.cuda()\n",
    "            return exit_loss\n",
    "        if self.exit_flag == \"BCE\":\n",
    "            exit_flag12 = torch.squeeze(exitflag[-1][2])\n",
    "            exit_flag13 = torch.squeeze(exitflag[-1][3])\n",
    "            exit_loss = (self.objective['exit_top'](exit_flag12, gt_exit) + self.objective['exit_top'](exit_flag13, gt_exit))/2\n",
    "            #exit_loss = self.objective['exit_bottom'](neg_flags, exitflag[-1][0]) + self.objective['exit_bottom'](neg_flags, exitflag[-1][1])\n",
    "        elif self.exit_flag == 'MATRIX_BCE':\n",
    "            exit_loss = self.cal_exit_prob(gt_bboxes_vec, exitflag[0], exitflag[1])\n",
    "        elif self.exit_flag == 'LAMBDA':\n",
    "\n",
    "            tmppos_tl = exitflag[-1][0][neg_flags==0]\n",
    "            tmppos_br = exitflag[-1][1][neg_flags==0]\n",
    "            if tmppos_tl.shape[0] ==0:\n",
    "                exit_loss = torch.tensor(0.0) #.cuda()\n",
    "            else:\n",
    "                exit_loss = torch.mean(tmppos_tl)\n",
    "            if tmppos_br.shape[0] ==0:\n",
    "                exit_loss += torch.tensor(0.0) #.cuda()\n",
    "            else:\n",
    "                exit_loss += torch.mean(tmppos_br)\n",
    "\n",
    "            tmpneg_tl = exitflag[-1][0][neg_flags==1] \n",
    "            tmpneg_br = exitflag[-1][1][neg_flags==1]\n",
    "            if tmpneg_tl.shape[0] ==0:\n",
    "                exit_loss -= torch.tensor(0.0) #.cuda()\n",
    "            else:\n",
    "                exit_loss -= torch.mean(tmpneg_tl)\n",
    "            if tmpneg_br.shape[0] ==0:\n",
    "                exit_loss -= torch.tensor(0.0) #.cuda()\n",
    "            else:\n",
    "                exit_loss -= torch.mean(tmpneg_br)\n",
    "        else:\n",
    "            raise Exception('Invalid exit loss')\n",
    "        return exit_loss\n",
    "\n",
    "    def make_gt_matrix(self, feat_sz, tgt_idx):\n",
    "        gt_matrix = torch.zeros(feat_sz*feat_sz).view(feat_sz, feat_sz) #.cuda()\n",
    "        for i in range(feat_sz):\n",
    "            for j in range(feat_sz):\n",
    "                gt_matrix[i][j] = torch.exp(-(j-tgt_idx[0])**2-(i-tgt_idx[1])**2)\n",
    "        gt_matrix = torch.squeeze(gt_matrix.view(-1, feat_sz*feat_sz))\n",
    "        return gt_matrix\n",
    "\n",
    "    def make_uni_matrix(self, feat_sz):\n",
    "        gt_matrix = torch.ones(feat_sz*feat_sz)/(feat_sz*feat_sz) #.cuda()\n",
    "\n",
    "        return gt_matrix\n",
    "    \n",
    "    def cal_exit_prob(self, gt_bboxes, prob_vec_tl, prob_vec_br):\n",
    "        feat_sz = 20\n",
    "        gt_bboxes = torch.squeeze(gt_bboxes)\n",
    "        index = torch.round(gt_bboxes*feat_sz).int()\n",
    "        ent_matrix_loss = 0\n",
    "        neg_tup = torch.nonzero(gt_bboxes<0, as_tuple=True)\n",
    "        neg_n = torch.unique(neg_tup[0])\n",
    "        neg_b = torch.unique(neg_tup[1])\n",
    "\n",
    "        if gt_bboxes.dim()==3:\n",
    "            n, b, _ = gt_bboxes.shape\n",
    "            for i in range(n):\n",
    "                for j in range(b):\n",
    "                    if i in neg_n and j in neg_b:\n",
    "                        gt_matrix = self.make_uni_matrix(feat_sz)\n",
    "                        ent_matrix_loss += self.objective['exit_top'](prob_vec_tl[i,j], gt_matrix)\n",
    "                        # print(\"GT UNI matrix\", gt_matrix)\n",
    "                        gt_matrix = self.make_uni_matrix(feat_sz)\n",
    "                        ent_matrix_loss += self.objective['exit_top'](prob_vec_br[i,j], gt_matrix)\n",
    "                    else:\n",
    "                        gt_matrix = self.make_gt_matrix(feat_sz, [index[i,j,0], index[i,j,1]])\n",
    "                        ent_matrix_loss += self.objective['exit_top'](prob_vec_tl[i,j], gt_matrix)\n",
    "                        # print(\"TL gaussian matrix\", gt_matrix)\n",
    "                        gt_matrix = self.make_gt_matrix(feat_sz, [index[i,j,2], index[i,j,3]])\n",
    "                        ent_matrix_loss += self.objective['exit_top'](prob_vec_br[i,j], gt_matrix)\n",
    "            ent_matrix_loss = ent_matrix_loss/(n*b)\n",
    "        else:\n",
    "            b, _ = gt_bboxes.shape\n",
    "\n",
    "            for j in range(b):\n",
    "                if j in neg_n:\n",
    "                    gt_matrix = self.make_uni_matrix(feat_sz)\n",
    "                    ent_matrix_loss += self.objective['exit_top'](prob_vec_tl[j], gt_matrix)\n",
    "                    gt_matrix = self.make_uni_matrix(feat_sz)\n",
    "                    ent_matrix_loss += self.objective['exit_top'](prob_vec_br[j], gt_matrix)\n",
    "                else:\n",
    "                    gt_matrix = self.make_gt_matrix(feat_sz, [index[j,0], index[j,1]])\n",
    "                    ent_matrix_loss += self.objective['exit_top'](prob_vec_tl[j], gt_matrix)\n",
    "                    gt_matrix = self.make_gt_matrix(feat_sz, [index[j,2], index[j,3]])\n",
    "                    ent_matrix_loss += self.objective['exit_top'](prob_vec_br[j], gt_matrix)\n",
    "            ent_matrix_loss = ent_matrix_loss/(b)\n",
    "        \n",
    "        return ent_matrix_loss\n",
    "\n",
    "    def compute_reid_loss(self, gt_bbox, pred_bboxes_vec, feature):\n",
    "        #compute re-id loss\n",
    "        if feature == None:\n",
    "            reid_loss = torch.tensor(0.0) #.cuda()\n",
    "            return reid_loss\n",
    "        if gt_bbox.dim()==3:\n",
    "            pred_bboxes_vec = torch.round(pred_bboxes_vec*20).int()\n",
    "            \n",
    "            #feat_sz\n",
    "            reid_loss = torch.tensor(0.0) #.cuda()\n",
    "\n",
    "            assert gt_bbox.dim() ==3\n",
    "            n, b, c, _, _= feature.shape\n",
    "\n",
    "            vacant1 = torch.zeros(1, 20, 20) #.cuda()\n",
    "            vacant2 = torch.zeros(1, 20, 20) #.cuda()        \n",
    "            for j in range(self.bs):\n",
    "                for i in range(n-1):            \n",
    "                    vacant11 = vacant1.repeat(c, 1, 1)\n",
    "                    vacant22 = vacant2.repeat(c, 1, 1)\n",
    "\n",
    "                    if gt_bbox[i, j, 1]>=0 and gt_bbox[i+1, j, 1]>=0:\n",
    "                        vacant11[:, pred_bboxes_vec[j, i+1, 1]:pred_bboxes_vec[j, i+1, 3],pred_bboxes_vec[j, i+1, 0]:pred_bboxes_vec[j, i+1, 2]] = 1  \n",
    "                        vacant22[:, pred_bboxes_vec[j, i, 1]:pred_bboxes_vec[j, i, 3],pred_bboxes_vec[j, i, 0]:pred_bboxes_vec[j, i, 2]] = 1  \n",
    "\n",
    "                        predN_Bbox = feature[i+1, j] * vacant11\n",
    "                        pred_Bbox = feature[i,j] * vacant22\n",
    "                        reid_loss += self.objective['reId'](pred_Bbox, predN_Bbox)\n",
    "                    else:\n",
    "                        reid_loss += torch.tensor(0.0) #.cuda()\n",
    "\n",
    "            return reid_loss/(n*b)\n",
    "        else:\n",
    "            reid_loss = torch.tensor(0.0) #.cuda()\n",
    "            return reid_loss\n",
    "\n",
    "    def get_optimizer_scheduler(self, cfg):\n",
    "        train_cls = getattr(cfg.TRAIN, \"TRAIN_CLS\", False)\n",
    "        # Adam(self.parameters(), lr=self.lr)\n",
    "        if train_cls:\n",
    "            # print(\"Only training classification head. Learnable parameters are shown below.\")\n",
    "            param_dicts = [\n",
    "                {\"params\": [p for n, p in self.net.named_parameters() if \"cls\" in n and p.requires_grad]}\n",
    "            ]\n",
    "\n",
    "            for n, p in self.net.named_parameters():\n",
    "                if \"cls\" not in n:\n",
    "                    p.requires_grad = False\n",
    "                # else:\n",
    "                #     print(n)\n",
    "        else:\n",
    "            param_dicts = [\n",
    "                {\"params\": [p for n, p in self.net.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.net.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                    \"lr\": self.lr * cfg.TRAIN.BACKBONE_MULTIPLIER,\n",
    "                },\n",
    "            ]\n",
    "            # if is_main_process():\n",
    "            #     print(\"Learnable parameters are shown below.\")\n",
    "            #     for n, p in self.net.named_parameters():\n",
    "            #         if p.requires_grad:\n",
    "            #             print(n)\n",
    "\n",
    "        if cfg.TRAIN.OPTIMIZER == \"ADAMW\":\n",
    "            optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                        weight_decay=cfg.TRAIN.WEIGHT_DECAY)\n",
    "            ## weight decay pick it out.\n",
    "        elif cfg.TRAIN.OPTIMIZER == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(param_dicts, lr=self.lr,\n",
    "                                        weight_decay=cfg.TRAIN.WEIGHT_DECAY)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported Optimizer\")\n",
    "        if cfg.TRAIN.SCHEDULER.TYPE == 'step':\n",
    "            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, cfg.TRAIN.LR_DROP_EPOCH)\n",
    "        elif cfg.TRAIN.SCHEDULER.TYPE == \"Mstep\":\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                                milestones=cfg.TRAIN.SCHEDULER.MILESTONES,\n",
    "                                                                gamma=cfg.TRAIN.SCHEDULER.GAMMA)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scheduler\")\n",
    "        return optimizer, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/ashleve/ac511f08c0d29e74566900fd3efbb3ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib.test.evaluation import get_dataset\n",
    "# from lib.test.evaluation.running import run_dataset\n",
    "# from lib.test.evaluation.tracker import Tracker\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class RobotDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir='./', k=1, split_seed=123, num_splits=10, batch_size=256, num_workers=8, pin_memory=False):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size  \n",
    "        self.k = k\n",
    "        self.num_splits = num_splits\n",
    "        self.split_seed = split_seed\n",
    "\n",
    "\n",
    "        # self.transform = transforms.ToTensor()\n",
    "\n",
    "    def fill_state(self, cfg, settings):\n",
    "        self.cfg = cfg\n",
    "        self.settings = settings\n",
    "\n",
    "        transform_joint = tfm.Transform(tfm.ToGrayscale(probability=0.05),\n",
    "                                        tfm.RandomHorizontalFlip(probability=0.5))\n",
    "\n",
    "        transform_train = tfm.Transform(tfm.ToTensorAndJitter(0.2),\n",
    "                                        tfm.RandomHorizontalFlip_Norm(probability=0.5),\n",
    "                                        tfm.Normalize(mean=cfg.DATA.MEAN, std=cfg.DATA.STD))\n",
    "\n",
    "        # transform_val = tfm.Transform(tfm.ToTensor(),\n",
    "        #                             tfm.Normalize(mean=cfg.DATA.MEAN, std=cfg.DATA.STD))\n",
    "\n",
    "        # The tracking pairs processing module\n",
    "        output_sz = settings.output_sz\n",
    "        search_area_factor = settings.search_area_factor\n",
    "\n",
    "        self.data_processing_train = processing.STARKProcessing(search_area_factor=search_area_factor,\n",
    "                                                        output_sz=output_sz,\n",
    "                                                        center_jitter_factor=settings.center_jitter_factor,\n",
    "                                                        scale_jitter_factor=settings.scale_jitter_factor,\n",
    "                                                        mode='sequence',\n",
    "                                                        transform=transform_train,\n",
    "                                                        joint_transform=transform_joint,\n",
    "                                                        settings=settings)\n",
    "\n",
    "        # self.data_processing_val = processing.STARKProcessing(search_area_factor=search_area_factor,\n",
    "        #                                                 output_sz=output_sz,\n",
    "        #                                                 center_jitter_factor=settings.center_jitter_factor,\n",
    "        #                                                 scale_jitter_factor=settings.scale_jitter_factor,\n",
    "        #                                                 mode='sequence',\n",
    "        #                                                 transform=transform_val,\n",
    "        #                                                 joint_transform=transform_joint,\n",
    "        #                                                 settings=settings)\n",
    "\n",
    "        # Train sampler and loader\n",
    "        settings.num_template = getattr(cfg.DATA.TEMPLATE, \"NUMBER\", 1)\n",
    "        settings.num_search = getattr(cfg.DATA.SEARCH, \"NUMBER\", 1)\n",
    "        self.sampler_mode = getattr(cfg.DATA, \"SAMPLER_MODE\", \"causal\")\n",
    "        self.train_cls = getattr(cfg.TRAIN, \"TRAIN_CLS\", False)\n",
    "        \n",
    "    # def prepare_data(self):\n",
    "    #     '''called only once and on 1 GPU'''\n",
    "    #     # download data\n",
    "    #     MNIST(self.data_dir, train=True, download=True)\n",
    "    #     MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''called on each GPU separately - stage defines if we are at fit or test step'''\n",
    "        # we set up only relevant datasets when stage is specified (automatically set by Pytorch-Lightning)\n",
    "        cfg = self.cfg\n",
    "        settings = self.settings\n",
    "        kfold = KFold(n_splits = self.num_splits, shuffle = True, random_state = self.split_seed)\n",
    "        \n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.dataset_full = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.TRAIN.DATASETS_NAME, settings, opencv_loader),\n",
    "                                            p_datasets=cfg.DATA.TRAIN.DATASETS_RATIO,\n",
    "                                            samples_per_epoch=cfg.DATA.TRAIN.SAMPLE_PER_EPOCH,\n",
    "                                            max_gap=cfg.DATA.MAX_SAMPLE_INTERVAL, num_search_frames=settings.num_search,\n",
    "                                            num_template_frames=settings.num_template, processing=self.data_processing_train,\n",
    "                                            frame_sample_mode=self.sampler_mode, batch_size=self.batch_size, train_cls=self.train_cls)\n",
    "            \n",
    "            # self.dataset_val = sampler.TrackingSampler(datasets=names2datasets(cfg.DATA.VAL.DATASETS_NAME, settings, opencv_loader),\n",
    "            #                               p_datasets=cfg.DATA.VAL.DATASETS_RATIO,\n",
    "            #                               samples_per_epoch=cfg.DATA.VAL.SAMPLE_PER_EPOCH,\n",
    "            #                               max_gap=cfg.DATA.MAX_SAMPLE_INTERVAL, num_search_frames=settings.num_search,\n",
    "            #                               num_template_frames=settings.num_template, processing=self.data_processing_val,\n",
    "            #                               frame_sample_mode=self.sampler_mode, batch_size=self.batch_size, train_cls=self.train_cls)\n",
    "\n",
    "            all_splits = [k for k in kfold.split(self.dataset_full)]\n",
    "            train_indexes, val_indexes = all_splits[self.k]\n",
    "            self.train_subsampler = torch.utils.data.SubsetRandomSampler(train_indexes)\n",
    "            self.val_subsampler = torch.utils.data.SubsetRandomSampler(val_indexes)\n",
    "\n",
    "        # if stage == 'test' or stage is None:\n",
    "        #     self.dataset_test = get_dataset(cfg.DATA.TEST.DATASETS_NAME)\n",
    "        #     pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        '''returns training dataloader'''\n",
    "        cfg = self.cfg        \n",
    "        loader_train = LTRLoader('train', self.dataset_full, training=True, sampler=self.train_subsampler, batch_size=self.batch_size, \n",
    "                             num_workers=cfg.TRAIN.NUM_WORKER, drop_last=True, stack_dim=1)\n",
    "        return loader_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        '''returns validation dataloader'''\n",
    "        cfg = self.cfg\n",
    "        loader_val = LTRLoader('val', self.dataset_full, training=False, sampler=self.val_subsampler, batch_size=self.batch_size,\n",
    "                           num_workers=cfg.TRAIN.NUM_WORKER, drop_last=True, stack_dim=1, \n",
    "                           epoch_interval=cfg.TRAIN.VAL_EPOCH_INTERVAL)\n",
    "\n",
    "        return loader_val\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     '''returns test dataloader'''\n",
    "    #     mnist_test = DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "    #     return mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.8 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages (from scikit-learn) (1.21.5)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.box_ops import giou_loss\n",
    "from torch.nn.functional import l1_loss\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "# train pipeline related\n",
    "from lib.train.trainers import LTRTrainer\n",
    "from lib.train.base_functions import *\n",
    "\n",
    "from lib.models.exot import build_exotst\n",
    "\n",
    "# forward propagation related\n",
    "from lib.train.actors import EXOTActor, EXOTSTActor\n",
    "import importlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')# good solution !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script_name: exot_st1.py  config_name: baseline_mix.yaml\n",
      "New configuration is shown below.\n",
      "MODEL configuration: {'HEAD_TYPE': 'CORNER', 'NLAYER_HEAD': 3, 'HEAD_ABS': False, 'HIDDEN_DIM': 256, 'NUM_OBJECT_QUERIES': 1, 'POSITION_EMBEDDING': 'sine', 'PREDICT_MASK': False, 'LOSS_TYPE': 'None', 'BACKBONE': {'TYPE': 'resnet50', 'OUTPUT_LAYERS': ['layer3'], 'STRIDE': 16, 'DILATION': False}, 'TRANSFORMER': {'NHEADS': 8, 'DROPOUT': 0.1, 'DIM_FEEDFORWARD': 2048, 'ENC_LAYERS': 6, 'DEC_LAYERS': 6, 'PRE_NORM': False, 'DIVIDE_NORM': False}}\n",
      "\n",
      "\n",
      "TRAIN configuration: {'LR': 0.0001, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 100, 'LR_DROP_EPOCH': 95, 'BATCH_SIZE': 16, 'NUM_WORKER': 0, 'OPTIMIZER': 'ADAMW', 'BACKBONE_MULTIPLIER': 0.1, 'REID_WEIGHT': 3.0, 'EXIT_WEIGHT': 3.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'DEEP_SUPERVISION': False, 'FREEZE_BACKBONE_BN': True, 'FREEZE_LAYERS': ['conv1', 'layer1'], 'PRINT_INTERVAL': 50, 'VAL_EPOCH_INTERVAL': 5, 'GRAD_CLIP_NORM': 0.1, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}\n",
      "\n",
      "\n",
      "DATA configuration: {'SAMPLER_MODE': 'trident_pro', 'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': [200], 'TRAIN': {'DATASETS_NAME': ['ROBOT_train', 'TREK150_train'], 'DATASETS_RATIO': [1, 1], 'SAMPLE_PER_EPOCH': 100000}, 'VAL': {'DATASETS_NAME': ['ROBOT_val', 'TREK150_val'], 'DATASETS_RATIO': [1, 1], 'SAMPLE_PER_EPOCH': 10000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 320, 'FACTOR': 5.0, 'CENTER_JITTER': 4.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 2, 'SIZE': 128, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(args = ['--script', 'exot_st1', '--config', 'baseline_mix', '--save_dir', '.', '--mode', 'single'])\n",
    "cv.setNumThreads(0)\n",
    "torch.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n",
    "print('script_name: {}.py  config_name: {}.yaml'.format(args.script, args.config))\n",
    "\n",
    "'''2021.1.5 set seed for different process'''\n",
    "if args.seed is not None:\n",
    "    if args.local_rank != -1:\n",
    "        init_seeds(args.seed + args.local_rank)\n",
    "    else:\n",
    "        init_seeds(args.seed)\n",
    "\n",
    "settings = Settings()\n",
    "cfg = settings.set_args(args)\n",
    "# update settings based on cfg\n",
    "if settings.local_rank in [-1, 0]:\n",
    "    print(\"New configuration is shown below.\")\n",
    "    for key in cfg.keys():\n",
    "        print(\"%s configuration:\" % key, cfg[key])\n",
    "        print('\\n')  \n",
    "update_settings(settings, cfg)\n",
    "\n",
    "'''\n",
    "results = []\n",
    "nums_folds = 10\n",
    "split_seed = 12345\n",
    "\n",
    "for k in range(nums_folds):\n",
    "    datamodule = ProteinsKFoldDataModule(k=k, num_folds=num_folds, split_seed=split_seed, ...)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup()\n",
    "\n",
    "    ...\n",
    "    # here we train the model on given split...\n",
    "    ...\n",
    "    results.append(score)\n",
    "\n",
    "score = sum(results) / num_folds\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "objective = {'giou': giou_loss, 'l1': l1_loss, 'joint': l1_loss, 'reId': l1_loss, 'exit_top': BCEWithLogitsLoss(), 'exit_bottom': l1_loss}\n",
    "\n",
    "loss_weight = {'giou': cfg.TRAIN.GIOU_WEIGHT, 'l1': cfg.TRAIN.L1_WEIGHT, 'reId': cfg.TRAIN.REID_WEIGHT, 'exit': cfg.TRAIN.EXIT_WEIGHT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head channel: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n    fn(i, *args)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 129, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1104, in _run\n    self.strategy.setup_environment()\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\", line 130, in setup_environment\n    self.accelerator.setup_environment(self.root_device)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/accelerators/cuda.py\", line 43, in setup_environment\n    torch.cuda.set_device(root_device)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 311, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 205, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_362223/2824384054.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLitEXOTActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOSS_TYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrobot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exot_st1_mix_model_%d.pth\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#self.log(..., batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         )\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         )\n\u001b[1;32m    109\u001b[0m         \u001b[0mworker_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n    fn(i, *args)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 129, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1104, in _run\n    self.strategy.setup_environment()\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\", line 130, in setup_environment\n    self.accelerator.setup_environment(self.root_device)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/pytorch_lightning/accelerators/cuda.py\", line 43, in setup_environment\n    torch.cuda.set_device(root_device)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 311, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/home/hskim/miniconda/envs/deformable_detr/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 205, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "nums_folds = 5\n",
    "for k in range(nums_folds):\n",
    "    wandb_logger = WandbLogger(project='EXOT', name=f\"fold_{k}\")\n",
    "    trainer = Trainer(logger=wandb_logger, accelerator='gpu', devices=-1, max_epochs=50)\n",
    "    robot_data = RobotDataModule(data_dir='data/robot-data/', k=k, num_splits=nums_folds, batch_size=16)\n",
    "    robot_data.fill_state(cfg, settings)\n",
    "    net = build_exotst(cfg)\n",
    "    model = LitEXOTActor(cfg, settings, loss_type=cfg.MODEL.LOSS_TYPE, lr=cfg.TRAIN.LR)\n",
    "    model.construct(net, objective, loss_weight)\n",
    "    trainer.fit(model, datamodule=robot_data)\n",
    "    model.save(os.path.join(wandb.run.dir, \"exot_st1_mix_model_%d.pth\"%k))\n",
    "    #self.log(..., batch_size=batch_size)\n",
    "    wandb.finish()\n",
    "    ## ODIN batchwise - classification diff objects possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('deformable_detr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ab99f9e7a4994d9a514462648dcc9cf2c1ee60ca8354c67de736bf242c155aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
